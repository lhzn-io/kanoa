{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2479283e",
   "metadata": {},
   "source": [
    "# Context Caching with Gemini 3 Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d32aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "\n",
    "from kanoa import AnalyticsInterpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b03a5",
   "metadata": {},
   "source": [
    "## Step 1: Locate Knowledge Base\n",
    "\n",
    "We'll use the example climate science knowledge base included in this repository.\n",
    "It contains markdown files covering CO2 emissions, ocean temperatures, and methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d036b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the example climate science knowledge base\n",
    "KB_PATH = (\n",
    "    Path(__file__).parent / \"knowledge_base_demo\" / \"climate_science_kb\"\n",
    "    if \"__file__\" in dir()\n",
    "    else Path(\"knowledge_base_demo/climate_science_kb\")\n",
    ")\n",
    "\n",
    "# List KB files\n",
    "kb_files = list(KB_PATH.glob(\"*.md\"))\n",
    "print(f\"Knowledge base path: {KB_PATH}\")\n",
    "print(f\"Files: {[f.name for f in kb_files]}\")\n",
    "\n",
    "# Show total content size\n",
    "total_chars = sum(f.read_text().count(\"\") for f in kb_files)\n",
    "print(f\"Total content: ~{sum(len(f.read_text()) for f in kb_files):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908a2d1",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Interpreter with Caching\n",
    "\n",
    "Create an `AnalyticsInterpreter` with context caching enabled.\n",
    "The `cache_ttl` parameter controls how long the cache persists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize interpreter with context caching\n",
    "# cache_ttl=3600 means the cache is valid for 1 hour (3600 seconds)\n",
    "\n",
    "interpreter = AnalyticsInterpreter(\n",
    "    backend=\"gemini-3\",\n",
    "    kb_path=str(KB_PATH),\n",
    "    cache_ttl=3600,  # 1 hour cache TTL\n",
    ")\n",
    "\n",
    "print(f\"Backend: {interpreter.backend}\")\n",
    "print(f\"Cache TTL: {interpreter._backend.cache_ttl}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e23820",
   "metadata": {},
   "source": [
    "## Step 3: First Query (Cache Miss)\n",
    "\n",
    "The first query will upload the knowledge base and create a cache.\n",
    "You'll see the full token cost for the KB content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea71ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First query - this creates the cache\n",
    "result1 = interpreter.interpret(\n",
    "    prompt=\"What is the current rate of CO2 increase per year?\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FIRST QUERY RESULTS (Cache Creation)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nResponse:\\n{result1.text[:500]}...\\n\")\n",
    "\n",
    "if result1.usage:\n",
    "    print(f\"Input tokens:  {result1.usage.input_tokens:,}\")\n",
    "    print(f\"Output tokens: {result1.usage.output_tokens:,}\")\n",
    "    print(f\"Cached tokens: {result1.usage.cached_tokens or 0:,}\")\n",
    "    print(f\"Cache savings: ${result1.usage.cache_savings:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa3a6c",
   "metadata": {},
   "source": [
    "## Step 4: Second Query (Cache Hit)\n",
    "\n",
    "The second query reuses the cached knowledge base.\n",
    "Notice the **cached tokens** are now non-zero, and **cache savings** shows the cost reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second query - this reuses the cache\n",
    "result2 = interpreter.interpret(\n",
    "    prompt=\"What are the sea level projections for 2100 under SSP5-8.5?\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SECOND QUERY RESULTS (Cache Hit)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nResponse:\\n{result2.text[:500]}...\\n\")\n",
    "\n",
    "if result2.usage:\n",
    "    print(f\"Input tokens:  {result2.usage.input_tokens:,}\")\n",
    "    print(f\"Output tokens: {result2.usage.output_tokens:,}\")\n",
    "    print(f\"Cached tokens: {result2.usage.cached_tokens or 0:,}\")\n",
    "    print(f\"Cache savings: ${result2.usage.cache_savings:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d04ed",
   "metadata": {},
   "source": [
    "## Understanding Cost Savings\n",
    "\n",
    "Context caching provides significant cost savings for repeated queries:\n",
    "\n",
    "| Token Type | Price per 1M tokens | Description |\n",
    "| --- | --- | --- |\n",
    "| Standard Input | $2.00 | Regular input tokens |\n",
    "| Cached Input | $0.50 | Tokens from cache |\n",
    "| Cache Storage | $0.20/hr | Per million cached tokens |\n",
    "\n",
    "**Savings formula**: `(cached_tokens / 1M) * ($2.00 - $0.50) = savings`\n",
    "\n",
    "For a 10,000 token knowledge base queried 10 times:\n",
    "- Without caching: `10 × 10,000 × $2.00/1M = $0.20`\n",
    "- With caching: `10,000 × $2.00/1M + 9 × 10,000 × $0.50/1M = $0.065`\n",
    "- **67% savings!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532310c0",
   "metadata": {},
   "source": [
    "## Step 5: Clear Cache (Optional)\n",
    "\n",
    "You can manually clear the cache if you've updated your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c862186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the cache to force a refresh\n",
    "interpreter.clear_cache()\n",
    "print(\"Cache cleared. Next query will create a new cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809f488",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "kanoa's context caching feature:\n",
    "\n",
    "1. **Automatically** caches your knowledge base content\n",
    "2. **Reuses** the cache for subsequent queries (same content hash)\n",
    "3. **Saves ~75%** on input token costs for cached content\n",
    "4. **Tracks** cached tokens and savings in the `UsageInfo` object\n",
    "\n",
    "### When to Use Context Caching\n",
    "\n",
    "- ✅ Interactive analysis sessions with multiple queries\n",
    "- ✅ Batch processing against a stable knowledge base\n",
    "- ✅ Knowledge bases > 2,048 tokens (minimum for caching benefit)\n",
    "\n",
    "### When NOT to Use Context Caching\n",
    "\n",
    "- ❌ Single-shot queries (cache creation overhead)\n",
    "- ❌ Rapidly changing knowledge bases\n",
    "- ❌ Very small knowledge bases (< 2,048 tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kanoa-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
